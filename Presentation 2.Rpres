Word RippeR     
========================================================
left: 60%
author: Winston A. Saunders
font-family: 'Helvetica'
date: April 19, 2015 
transition: rotate
   
  

```{r, echo=FALSE}
setwd("/Users/winstonsaunders/Documents/Datascience_captsone_presentation/")
```
***
![alt text](tactical.001.jpg)


Word Ripper Use Instructions
========================================================
 
__Toolkit:__      
- Corpus-RippeR:  <small>_creates computable text samples._</small>    
- n-gram-creatoR:  <small>_builds n-gram frequency tables_</small>  
- n-gram-reduceR:   <small>_combines & reduces mulitple n-gram tables._</small>

__Web Interface:__
- Word-RippeR:  <small>[Web based interface](https://ww44ss.shinyapps.io/Coursera_Shiny_Capstone/) _providing_ __context__ _or_ __nearest word__ _based predictions._</small>    
  
<center>__Agile, Flexible, and Compact Natural Language Prediction__</center>
        
Algorithm Description
========================================================

The __Word-Match Algorithm__ as the following steps:  
1. Extract last three words from text string.  
2. Count matches to four-gram "stems".  
3. Repeat for three- and two-grams.  
4. Calculate conditional probabilities and sort results.   
5. Select highest probability/highest order matched n-gram as best match.  

The __Context Match Algorithm__ works similarly, except stop words are removed from text and n-grams processing. 

Markov n-gram look-up tables
========================================================

Basing prediction on n-gram frequencies stored as integers

```{r, eval=FALSE}
freq <- as.integer(2000*log10(word_count))
```

give faster look-up based conditional probability

```{r, echo=FALSE, results='asis'}
directory <- "/Users/winstonsaunders/Documents/Coursera Swiftkey Data/final/en_US/markov_n_grams/"
three_gram <-read.csv(paste0(directory, "markov_three_gram.csv"))
```

```{r, echo=FALSE, results='asis' }
require(xtable)
three_gram$X.1<-NULL
three_gram$X<-NULL
print(xtable(three_gram[sample(50:400, 4),]), type="html", include.rownames = FALSE)
```

```{r, eval=FALSE}
log_cond_prob <- frequency - root_freq
```

Example Results
=============================================

```{r, echo=FALSE}
        directory <- "/Users/winstonsaunders/Documents/Coursera Swiftkey Data/final/en_US/markov_n_grams/"
 

        four_gram  <<- read.csv(paste0(directory, "markov_four_gram.csv"))
        three_gram <<-read.csv(paste0(directory, "markov_three_gram.csv"))
        two_gram   <<-read.csv(paste0(directory, "markov_two_gram.csv"))
        one_gram   <<-read.csv(paste0(directory, "markov_one_gram.csv"))
        
        three_gram_X <<- read.csv(paste0(directory, "markov_three_gram_X.csv"))
        two_gram_X   <<- read.csv(paste0(directory, "markov_two_gram_X.csv"))
        one_gram_X   <<-read.csv(paste0(directory, "markov_one_gram_X.csv"))


        last_three_words <- function(x = "you didnt enter anything") {
                
                x<- gsub("[[:punct:]]", "", x) 
                
                text_a<-strsplit(x, " ")[[1]]  ## get the actual words
                i = length(text_a)
                
                if (i>=1) {y<-paste(text_a[i])
                } else y<-"blank"
                
                if (i>=2) {y<-paste(text_a[i-1],text_a[i])
                }
                
                if (i>=3) {y<-paste(text_a[i-2],text_a[i-1],text_a[i])
                }
                
                
                
                return(y)
        }

## PREDICTION FUNCTION BASED ON WORD N-GRAMS
        predictor_word <- function(full_text = "my music is") {
                
                
                full_text_words<-strsplit(tolower(full_text), " ")[[1]]
                
                text<-last_three_words(full_text)
                text_split<<-strsplit(text, " ")[[1]]  ## get the actual words
                
                ## Split text into individual words
                
                len2<-length(full_text_words)
                len<-length(text_split)
 
                
                ## Match words 
                i = min(3,len)
                

                ppflag<-1
                if (i==3) {text_4<-paste(text_split[len-2],text_split[len-1],text_split[len])
                           
                           pp <- four_gram[four_gram$stem==text_4,]
                           if(dim(pp)[1]==0) ppflag<-0
                           
                } else ppflag<-0
                
   
                qflag<-1
                if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
                           q <- three_gram[three_gram$stem==text_3,]
                           if (dim(q)[1]==0) qflag<-0
                } else qflag<-0
                        
                
                
                rflag<-1
                if (i>=1) {text_2<-paste(text_split[len])
                           
                           r <- two_gram[two_gram$stem==text_2,]
                           if (dim(r)[1]==0) rflag<-0
                } else rflag<-0
                
                
                
                ## default predicted word
                ## if all else fails flip a coin between most probable words
                if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"the" else 
                        if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"to" else 
                                predicted_word<-"and"
            
                # Use back-off to tune predict word choice                 
                if (rflag==1){
                        r$cond<-r$frequency - r$root_freq
                        r<-r[order(r$cond, decreasing=TRUE),]
                        predicted_word<-r$root[1]
                        predicted_phrase<-r[1:min(8, dim(r)[1]),]
                        
                        
                }

                if (qflag==1){
                        q$cond<-q$frequency - q$root_freq
                        q<-q[order(r$cond, decreasing=TRUE),]
                        predicted_word<-q$root[1]
                        predicted_phrase<-q[1:min(8, dim(q)[1]),]
                        
                }


                if (ppflag==1){
                        pp$cond<-pp$frequency - pp$root_freq
                        pp<-pp[order(pp$cond, decreasing=TRUE),]
                        predicted_word<-pp$root[1]
                        predicted_phrase<-pp[1:min(8, dim(pp)[1]),]

                }
                
                return(predicted_phrase)
        }

## PREDICTION FUNCTION BASED ON NON-STOP WORDS (Context)
##
        predictor_function <- function(full_text = "my music is") {
                full_text_words<-strsplit(tolower(full_text), " ")[[1]]

                text_split<-full_text_words[full_text_words %in% one_gram_X$n_gram]
        
                ## Split text into individual words
                
        
                len<-length(text_split)
                
                
                ## Match words 
                i = min(2,len)
                
                qflag<-1
                if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
                           q <- three_gram_X[three_gram_X$stem==text_3,]
                           if (dim(q)[1]==0) qflag<-0
                }
                else qflag<-0
                
                
                rflag<-1
                if (i>=1) {text_2<-paste(text_split[len])
                           r <- two_gram_X[two_gram_X$stem==text_2,]
                           if (dim(r)[1]==0) rflag<-0
                }
                else rflag<-0
                
                
                ## default predicted word
                ## if all else fails flip a coin between most probable words
                if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"will"
                else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"said"
                else predicted_word<-"just"
                
                # Use back-off to tune predict word choice 
                
                if (rflag==1){
                        r$cond<-r$frequency - r$root_freq
                        r<-r[order(r$cond, decreasing=TRUE),]
                        predicted_word<-r$root[1]
                        predicted_phrase<-r[1:min(8, dim(r)[1]),]
                        
                        
                }
        
                
                if (qflag==1){
                        q$cond<-q$frequency - q$root_freq
                        q<-q[order(r$cond, decreasing=TRUE),]
                        predicted_word<-q$root[1]
                        predicted_phrase<-q[1:min(8, dim(q)[1]),]
                        
                }
                
                return(predicted_phrase)
        }
                


sample_phrase <- "Theres a lady who's sure all that glitters is gold and"

a<-predictor_word(sample_phrase)
b<-predictor_function(sample_phrase)

a<-a[complete.cases(a),]
b<-b[complete.cases(b),]

astore<-a
bstore<-b

colnames(astore)[8]<-"log_cond_prob"
colnames(bstore)[8]<-"log_cond_prob"



a$X.1<-NULL
a$X<-NULL
a$frequency<-NULL
a$root_freq<-NULL
b$X.1<-NULL
b$X<-NULL
b$frequency<-NULL
b$root_freq<-NULL

colnames(a)[4]<-"log_cond_prob"
colnames(b)[4]<-"log_cond_prob"

a<-a[1:3,]
b<-b[1:3,]


```
  
__phrase__: `r sample_phrase` ...   
  ...`r a$root[1]`  __(word based prediction)__    
  ...`r b$root[1]`  __(context based prediction)__   

Prediction relies on different stem depending on matching method

```{r, echo=FALSE, results='asis' }
require(xtable)

#print(xtable(as.data.frame(a)), type="html", include.rownames = FALSE)
#print(xtable(as.data.frame(b)), type="html", include.rownames = FALSE)

```


```{r, echo=FALSE,fig.width=12, fig.height=6, fig.show='hold', fig.align='center'}
library(ggplot2)

abstore<-rbind(astore, bstore)

p <- ggplot(abstore, aes(x=root, y = log_cond_prob, fill=(root_freq)))
p <- p + geom_bar(stat="identity") + coord_flip()
p <- p + facet_grid(.~stem)
#p <- p + scale_color_gradient2(midpoint=-1000, low="blue", mid="white", high="red" )
p <- p + ggtitle(paste("conditional probs for word- and context- prediction"))
p

```


