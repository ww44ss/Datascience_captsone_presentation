tactical.Data Word RippeR
========================================================
left: 70%
author: Winston A Saunders  
date: April 19, 2015 

- _Scalable_ to large corpora 
- _Reducible_ accelerated processing.
- _Selectable_ for context- or word-accuracy

***
```{r, echo=FALSE}
setwd("/Users/winstonsaunders/Documents/Datascience_captsone_presentation/")
```
![alt text](tactical.001.jpg)

Word Ripper Use Instructions
========================================================

Toolkit:       
- Corpus-RippeR: _creates computable uniform text samples from very large corpora._    
- n-gram-creatoR: _builds individual n-gram frequency tables_  
- n-gram-reduceR: _combines & reduces mulitple n-gram tables and computes conditional probabilities._

Web Interface:
- Word-RippeR _[Web based interface](https://ww44ss.shinyapps.io/Coursera_Shiny_Capstone/) providing_ __context__ _or nearest_ __word__ _based predictions._  
  
__Provides Agile, flexible, and compact Natural Language Prediction.__
        
Algorithm Description
========================================================

The __Word-Match Algorithm__ as the following steps:  
1. Extract last three words from text string.  
2. Count matches to four-gram "stems".  
3. Repeat for three- and two-grams.  
4. Calculate conditional probabilities and sort results.   
5. Select highest probability/highest order matched n-gram as best match.  

The __Context Match Algorithm__ works similarly, except stop words are removed from the n-grams. 

Compact Markov n_grams
========================================================

Prediction is based on n_gram frequencies stored as
```{r, echo=FALSE}
word_count<-1
```

```{r}
freq <- as.integer(2000*log10(word_count))
```

Faster interger addition and subraction

```{r, echo=FALSE, results='asis'}
directory <- "/Users/winstonsaunders/Documents/Coursera Swiftkey Data/final/en_US/markov_n_grams/"
three_gram <-read.csv(paste0(directory, "markov_three_gram.csv"))
```

```{r, echo=FALSE, results='asis' }
require(xtable)
three_gram$X.1<-NULL
three_gram$X<-NULL
print(xtable(three_gram[51:56,]), type="html")
```


Example Results
=============================================

```{r, echo=FALSE}
        directory <- "/Users/winstonsaunders/Documents/Coursera Swiftkey Data/final/en_US/markov_n_grams/"
 

        four_gram  <<- read.csv(paste0(directory, "markov_four_gram.csv"))
        three_gram <<-read.csv(paste0(directory, "markov_three_gram.csv"))
        two_gram   <<-read.csv(paste0(directory, "markov_two_gram.csv"))
        one_gram   <<-read.csv(paste0(directory, "markov_one_gram.csv"))
        
        three_gram_X <<- read.csv(paste0(directory, "markov_three_gram_X.csv"))
        two_gram_X   <<- read.csv(paste0(directory, "markov_two_gram_X.csv"))
        one_gram_X   <<-read.csv(paste0(directory, "markov_one_gram_X.csv"))


        last_three_words <- function(x = "you didnt enter anything") {
                
                x<- gsub("[[:punct:]]", "", x) 
                
                text_a<-strsplit(x, " ")[[1]]  ## get the actual words
                i = length(text_a)
                
                if (i>=1) {y<-paste(text_a[i])
                } else y<-"blank"
                
                if (i>=2) {y<-paste(text_a[i-1],text_a[i])
                }
                
                if (i>=3) {y<-paste(text_a[i-2],text_a[i-1],text_a[i])
                }
                
                
                
                return(y)
        }

## PREDICTION FUNCTION BASED ON WORD N-GRAMS
        predictor_word <- function(full_text = "my music is") {
                
                
                full_text_words<-strsplit(tolower(full_text), " ")[[1]]
                
                text<-last_three_words(full_text)
                text_split<<-strsplit(text, " ")[[1]]  ## get the actual words
                
                ## Split text into individual words
                
                len2<-length(full_text_words)
                len<-length(text_split)
 
                
                ## Match words 
                i = min(3,len)
                

                ppflag<-1
                if (i==3) {text_4<-paste(text_split[len-2],text_split[len-1],text_split[len])
                           
                           pp <- four_gram[four_gram$stem==text_4,]
                           if(dim(pp)[1]==0) ppflag<-0
                           
                }
                else ppflag<-0
                
   
                qflag<-1
                if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
                           q <- three_gram[three_gram$stem==text_3,]
                           if (dim(q)[1]==0) qflag<-0
                }
                else qflag<-0
                        
                
                
                rflag<-1
                if (i>=1) {text_2<-paste(text_split[len])
                           
                           r <- two_gram[two_gram$stem==text_2,]
                           if (dim(r)[1]==0) rflag<-0
                }
                else rflag<-0
                
                ## default predicted word
                ## if all else fails flip a coin between most probable words
                if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"the"
                    else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"to"
                        else predicted_word<-"and"
            
                # Use back-off to tune predict word choice                 
                if (rflag==1){
                        r$cond<-r$frequency - r$root_freq
                        r<-r[order(r$cond, decreasing=TRUE),]
                        predicted_word<-r$root[1]
                        predicted_phrase<-r[1:3,]
                        
                        
                }

                if (qflag==1){
                        q$cond<-q$frequency - q$root_freq
                        q<-q[order(r$cond, decreasing=TRUE),]
                        predicted_word<-q$root[1]
                        predicted_phrase<-q[1:3,]
                        
                }


                if (ppflag==1){
                        pp$cond<-pp$frequency - pp$root_freq
                        pp<-pp[order(pp$cond, decreasing=TRUE),]
                        predicted_word<-pp$root[1]
                        predicted_phrase<-pp[1:3,]

                }
                
                return(predicted_phrase)
        }

## PREDICTION FUNCTION BASED ON NON-STOP WORDS (Context)
##
        predictor_function <- function(full_text = "my music is") {
                full_text_words<-strsplit(tolower(full_text), " ")[[1]]

                text_split<-full_text_words[full_text_words %in% one_gram_X$n_gram]
        
                ## Split text into individual words
                
        
                len<-length(text_split)
                
                
                ## Match words 
                i = min(2,len)
                
                qflag<-1
                if (i>=2) {text_3<-paste(text_split[len-1],text_split[len])
                           q <- three_gram_X[three_gram_X$stem==text_3,]
                           if (dim(q)[1]==0) qflag<-0
                }
                else qflag<-0
                
                
                rflag<-1
                if (i>=1) {text_2<-paste(text_split[len])
                           r <- two_gram_X[two_gram_X$stem==text_2,]
                           if (dim(r)[1]==0) rflag<-0
                }
                else rflag<-0
                
                
                ## default predicted word
                ## if all else fails flip a coin between most probable words
                if (rbinom(n=1,size=1,prob=0.35) ==1) predicted_word<-"will"
                else if (rbinom(n=1,size=1,prob=0.52) ==1) predicted_word<-"said"
                else predicted_word<-"just"
                
                # Use back-off to tune predict word choice 
                
                if (rflag==1){
                        r$cond<-r$frequency - r$root_freq
                        r<-r[order(r$cond, decreasing=TRUE),]
                        predicted_word<-r$root[1]
                        predicted_phrase<-r[1:3,]
                        
                        
                }
        
                
                if (qflag==1){
                        q$cond<-q$frequency - q$root_freq
                        q<-q[order(r$cond, decreasing=TRUE),]
                        predicted_word<-q$root[1]
                        predicted_phrase<-q[1:3,]
                        
                }
                
                return(predicted_phrase)
        }
                


sample_phrase <- "You cook the best food I have ever "

a<-predictor_word(sample_phrase)
b<-predictor_function(sample_phrase)

a$X.1<-NULL
a$X<-NULL
a$frequency<-NULL
a$root_freq<-NULL
b$X.1<-NULL
b$X<-NULL
b$frequency<-NULL
b$root_freq<-NULL

colnames(a)[4]<-"log conditional prob"
colnames(b)[4]<-"log conditional prob"

```
  
__phrase__: `r sample_phrase` ...   
  ...`r a$root[1]`  __(word based prediction)__    
  ...`r b$root[1]`  __(context based prediction)__   


```{r, echo=FALSE, results='asis' }
require(xtable)

print(xtable(as.data.frame(a)), type="html")
print(xtable(as.data.frame(b)), type="html")

```
